{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "from os import getcwd\n",
    "import pathlib\n",
    "import datetime\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam, SGD\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of below is splitting images to use as keras.flow_from_directory\n",
    "fromDir = \"../input/microsoft-catsvsdogs-dataset/PetImages\"\n",
    "toDir = \"/kaggle/working/CatsAndDogs/Pet_images_new\"\n",
    "\n",
    "copy_tree(fromDir, toDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12501\n",
      "12501\n"
     ]
    }
   ],
   "source": [
    "current_working_dir = os.getcwd()\n",
    "\n",
    "path_to_images = pathlib.Path(os.path.join(current_working_dir, 'CatsAndDogs','Pet_images_new'))\n",
    "\n",
    "#current_working_dir = pathlib.Path('..','input','microsoft-catsvsdogs-dataset') #os.getcwd()\n",
    "#path_to_images = pathlib.Path(os.path.join(current_working_dir, 'PetImages'))\n",
    "Dogs_dir = os.path.join(path_to_images, 'Dog')\n",
    "Cats_dir = os.path.join(path_to_images, 'Cat')\n",
    "\n",
    "print(len(os.listdir(Dogs_dir)))\n",
    "print(len(os.listdir(Cats_dir)))\n",
    "\n",
    "\n",
    "dog_image_fnames = os.listdir(Dogs_dir)\n",
    "cat_image_fnames = os.listdir(Cats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 1590 images\n"
     ]
    }
   ],
   "source": [
    "num_skipped = 0\n",
    "for folder_name in (\"Cat\", \"Dog\"):\n",
    "    folder_path = os.path.join(path_to_images, folder_name)\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = os.path.join(folder_path, fname)\n",
    "        try:\n",
    "            fobj = open(fpath, \"rb\")\n",
    "            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n",
    "        finally:\n",
    "            fobj.close()\n",
    "\n",
    "        if not is_jfif:\n",
    "            num_skipped += 1\n",
    "            # Delete corrupted image\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(\"Deleted %d images\" % num_skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/CatsAndDogs/Pet_images_new/Dog/Thumbs.db\n",
      "/kaggle/working/CatsAndDogs/Pet_images_new/Cat/Thumbs.db\n",
      "23410\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "j=0\n",
    "for catagory in os.listdir(path_to_images):\n",
    "    sub_dir = os.path.join(path_to_images, catagory)\n",
    "    for name in os.listdir(sub_dir):\n",
    "        fpath = os.path.join(sub_dir,name)\n",
    "        if name.split('.')[1] == 'jpg':\n",
    "            if os.path.getsize(fpath) <= 0:\n",
    "                if os.path.isfile(fpath) == False:\n",
    "                    print(fpath)\n",
    "                os.remove(fpath)\n",
    "            elif os.path.getsize(fpath) > 0:\n",
    "                j +=1\n",
    "        if name.split('.')[1] != 'jpg':\n",
    "            print(fpath)\n",
    "            os.remove(fpath)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names_of_bad_images = ['5673.jpg','4688.jpg','12376.jpg','1043.jpg','8456.jpg',\n",
    "                      '9517.jpg','7377.jpg','1773.jpg','8736.jpg','10712.jpg','7564.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23410\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(path_to_images.glob('*/*.jpg')))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23410 files belonging to 2 classes.\n",
      "Using 18728 files for training.\n",
      "Found 23410 files belonging to 2 classes.\n",
      "Using 4682 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64  # max 1000 of these images can fit in gpu memory\n",
    "new_img_size = (200,200)\n",
    "data_split = 0.2 #0.0005\n",
    "seed_value = 42\n",
    "\n",
    "train_it = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    path_to_images,\n",
    "    validation_split=data_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed_value,\n",
    "    image_size=new_img_size,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "test_it = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    path_to_images,\n",
    "    validation_split=data_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed_value,\n",
    "    image_size=new_img_size,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, MaxPool2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Reshape\n",
    "def create_model():\n",
    "    #VGG-4 model with dropout\n",
    "    model = Sequential()\n",
    "    model.add(layers.experimental.preprocessing.Rescaling(1./255,input_shape=(new_img_size[0],new_img_size[1],3)))\n",
    "    model.add(tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'))\n",
    "    model.add(tf.keras.layers.experimental.preprocessing.RandomRotation(0.3))\n",
    "    # block 1\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # block 2\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # block 3\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # block 4\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # block 5\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # last layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "293/293 [==============================] - 47s 153ms/step - loss: 1.1230 - accuracy: 0.5371 - val_loss: 0.7427 - val_accuracy: 0.5045\n",
      "Epoch 2/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.6309 - accuracy: 0.6123 - val_loss: 0.5924 - val_accuracy: 0.6653\n",
      "Epoch 3/25\n",
      "293/293 [==============================] - 45s 150ms/step - loss: 0.6132 - accuracy: 0.6210 - val_loss: 0.5949 - val_accuracy: 0.6739\n",
      "Epoch 4/25\n",
      "293/293 [==============================] - 44s 150ms/step - loss: 0.5847 - accuracy: 0.6464 - val_loss: 0.5820 - val_accuracy: 0.7114\n",
      "Epoch 5/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.5687 - accuracy: 0.6927 - val_loss: 0.6486 - val_accuracy: 0.6820\n",
      "Epoch 6/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.5305 - accuracy: 0.7256 - val_loss: 0.6314 - val_accuracy: 0.6559\n",
      "Epoch 7/25\n",
      "293/293 [==============================] - 44s 150ms/step - loss: 0.5004 - accuracy: 0.7444 - val_loss: 0.4925 - val_accuracy: 0.7527\n",
      "Epoch 8/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.4885 - accuracy: 0.7663 - val_loss: 0.4693 - val_accuracy: 0.7659\n",
      "Epoch 9/25\n",
      "293/293 [==============================] - 45s 150ms/step - loss: 0.4535 - accuracy: 0.7873 - val_loss: 0.4133 - val_accuracy: 0.8120\n",
      "Epoch 10/25\n",
      "293/293 [==============================] - 45s 152ms/step - loss: 0.4439 - accuracy: 0.7956 - val_loss: 0.3592 - val_accuracy: 0.8319\n",
      "Epoch 11/25\n",
      "293/293 [==============================] - 45s 150ms/step - loss: 0.4273 - accuracy: 0.8107 - val_loss: 0.3476 - val_accuracy: 0.8496\n",
      "Epoch 12/25\n",
      "293/293 [==============================] - 45s 153ms/step - loss: 0.4087 - accuracy: 0.8197 - val_loss: 0.4991 - val_accuracy: 0.7625\n",
      "Epoch 13/25\n",
      "293/293 [==============================] - 46s 154ms/step - loss: 0.3870 - accuracy: 0.8315 - val_loss: 0.4579 - val_accuracy: 0.7774\n",
      "Epoch 14/25\n",
      "293/293 [==============================] - 45s 152ms/step - loss: 0.3733 - accuracy: 0.8350 - val_loss: 0.3649 - val_accuracy: 0.8313\n",
      "Epoch 15/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.3684 - accuracy: 0.8449 - val_loss: 0.2784 - val_accuracy: 0.8832\n",
      "Epoch 16/25\n",
      "293/293 [==============================] - 45s 152ms/step - loss: 0.3629 - accuracy: 0.8445 - val_loss: 0.2890 - val_accuracy: 0.8742\n",
      "Epoch 17/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.3502 - accuracy: 0.8514 - val_loss: 0.3444 - val_accuracy: 0.8507\n",
      "Epoch 18/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.3438 - accuracy: 0.8490 - val_loss: 0.2673 - val_accuracy: 0.8804\n",
      "Epoch 19/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.3265 - accuracy: 0.8600 - val_loss: 0.3143 - val_accuracy: 0.8620\n",
      "Epoch 20/25\n",
      "293/293 [==============================] - 45s 150ms/step - loss: 0.3218 - accuracy: 0.8629 - val_loss: 0.2343 - val_accuracy: 0.9047\n",
      "Epoch 21/25\n",
      "293/293 [==============================] - 45s 152ms/step - loss: 0.3211 - accuracy: 0.8659 - val_loss: 0.2340 - val_accuracy: 0.8964\n",
      "Epoch 22/25\n",
      "293/293 [==============================] - 44s 150ms/step - loss: 0.3131 - accuracy: 0.8668 - val_loss: 0.3274 - val_accuracy: 0.8648\n",
      "Epoch 23/25\n",
      "293/293 [==============================] - 45s 151ms/step - loss: 0.3171 - accuracy: 0.8643 - val_loss: 0.2389 - val_accuracy: 0.8964\n",
      "Epoch 24/25\n",
      "293/293 [==============================] - 45s 152ms/step - loss: 0.2954 - accuracy: 0.8807 - val_loss: 0.3133 - val_accuracy: 0.8682\n",
      "Epoch 25/25\n",
      "293/293 [==============================] - 45s 150ms/step - loss: 0.2978 - accuracy: 0.8750 - val_loss: 0.2343 - val_accuracy: 0.9024\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "stop_loss = EarlyStopping(monitor='val_loss', patience=10)\n",
    "epochs = 50\n",
    "history = model.fit(\n",
    "    train_it,\n",
    "    validation_data=test_it,\n",
    "    epochs=epochs//2,\n",
    "    verbose=1,\n",
    "    callbacks=[stop_loss]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
